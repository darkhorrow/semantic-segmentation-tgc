{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d9fa65",
   "metadata": {},
   "source": [
    "# Test custom Faster RCNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e100b06",
   "metadata": {},
   "source": [
    "Test the custom Faster RCNN provided by the tutors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4692428",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11b8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout, TimeDistributed, Layer\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b785eb4",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93517a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'F:/TFG-PabloHernandez-Detector/Modelos/TFG/ModeloTGCfull'\n",
    "TEST_IMAGES = '../TestsTGC/testArucas'\n",
    "RESULTS = 'F:/Datasets/FasterRCNNResults/'\n",
    "CONFIG = os.path.join(BASE_PATH, 'model/model_vgg_config.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e02fe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"TestsTGC/testArucas\" # Used as auxilair constant ~ match the TEST_IMAGES path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845a3f",
   "metadata": {},
   "source": [
    "## Classes and functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f923f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Config setting\n",
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Tamaños de anchores\n",
    "        self.anchor_box_scales = [32, 64, 128]\n",
    "\n",
    "        # Ratios de anchores\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Tamaño a redimensionar la dimension más pequeña de la imagen\n",
    "        self.im_size = 600\n",
    "\n",
    "        # numero de ROIs procesados simultáneamente\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride para el modelo RPN (modelo base VGG16)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "        # threshold para el modelo RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "\n",
    "        # threshold para el clasificador final\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # codificación de las clases\n",
    "        self.class_mapping = None\n",
    "\n",
    "        self.model_path = None\n",
    "\n",
    "#### Definicion ROI Pooling Convolutional Layer\n",
    "class RoiPoolingConv(Layer):\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "        self.dim_ordering = K.image_data_format()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        outputs = []\n",
    "        for roi_idx in range(self.num_rois):\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size, 'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "#### Vgg-16 modelo base\n",
    "def nn_base(input_tensor=None):\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=(None, None, 3))\n",
    "    else:\n",
    "        img_input = input_tensor\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "####  modelo RPN\n",
    "def rpn_layer(base_layers, num_anchors):\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]\n",
    "\n",
    "####  modelo clasificador final\n",
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes):\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # TimeDistributed layers se utiliza para procesar ROIs de forma independiente.\n",
    "    # Se indica el número de ROIs de entrada añadiendo una dimensión mas (num_rois)\n",
    "    # out_roi_pool es una lista de 4 RoI (7x7x512)\n",
    "    # out_roi_pool.shape = (1, num_rois, pool_size, pool_size, channels)\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten out_roi_pool y conectar a 2 Fully-Connected y 2 dropout layers\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    # out_class: prediccion de la clase del objeto\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # out_regr: prediccion de las coordenadas de los bboxes\n",
    "    out_regr = TimeDistributed(Dense(4*(nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]\n",
    "\n",
    "# Algoritmo NMS para evitar duplicidades en los bboxes delimitando un mismo objeto\n",
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "    # codigo extraido de: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    # Explicacion del proceso:\n",
    "    #   Paso 1: Ordenar la lista de probabilidades\n",
    "    #   Paso 2: Seleccionar la probabilidad más alta y copiarla en una lista aparte\n",
    "    #   Paso 3: Calcular el IoU entre el bbox de la probabilidad seleccionada con el resto de bboxes en la lista\n",
    "    #           Si (IoU > overlap_threshold) eliminar el bbox y probabilidad de su lista correspondiente\n",
    "    #   Paso 4: Repetir los pasos 2 y 3 hasta vaciar la lista de probabilidades\n",
    "\n",
    "    # si no hay bboxes devuelve una lista vacia\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # captura las coordenadas de todos los bboxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    np.testing.assert_array_less(x1, x2)\n",
    "    np.testing.assert_array_less(y1, y2)\n",
    "\n",
    "    # las coordenadas de los bboxes son convertidas a floats para las divisiones\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # lista de indices seleccionados\n",
    "    pick = []\n",
    "\n",
    "    # calculo de las areas de todos los bboxes\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # ordena las probabilidades (scores) de los bboxes en orden ascendente\n",
    "    # el score más alto está el último\n",
    "    idxs = np.argsort(probs)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        # añade el último index (el de mayor score) de la lista \"idx\" a la lista \"pick\"\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # busca las coordenadas más grandes (xmin,ymin) del top-left de cada bbox y\n",
    "        # las más grandes (xmax,ymax) del bottom-right de cada bbox\n",
    "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        # calcular el ancho y alto de cada bbox\n",
    "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "        # calcula la interseccion y la union\n",
    "        area_int = ww_int * hh_int\n",
    "        area_union = area[i] + area[idxs[:last]] - area_int\n",
    "        # calcula el IoU\n",
    "        overlap = area_int/(area_union + 1e-6)\n",
    "\n",
    "        # elimina los indices de la lista \"idx\" con IoU > overlap_thresh, y el último index tambien\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "        if len(pick) >= max_boxes:\n",
    "            break\n",
    "\n",
    "    # devuelve aquellos bboxes seleccionados, cuyos index están almacenados en la lista \"pick\"\n",
    "    boxes = boxes[pick].astype(\"int\")\n",
    "    probs = probs[pick]\n",
    "    return boxes, probs\n",
    "\n",
    "# aplica la correccion de los deltas predichos por el modelo RPN\n",
    "def apply_regr_rpn(X, T):\n",
    "    # corrige las coordenadas (x,y,w,h) del anchor según los deltas (tx,ty,tw,th)\n",
    "    # Segun se indica en el paper original:\n",
    "    # tx=(cx_gt-cx_anchor)/w_anchor, ty=(cy_gt-cy_anchor)/h_anchor, tw=log(w_gt/w_anchor), tw=log(h_gt/h_anchor)\n",
    "    # Nota: np.exp() permite trabajar con arrays, mientras que math.exp() sólo con escalares\n",
    "    try:\n",
    "        x = X[0, :, :]\n",
    "        y = X[1, :, :]\n",
    "        w = X[2, :, :]\n",
    "        h = X[3, :, :]\n",
    "\n",
    "        tx = T[0, :, :]\n",
    "        ty = T[1, :, :]\n",
    "        tw = T[2, :, :]\n",
    "        th = T[3, :, :]\n",
    "\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "\n",
    "        w1 = np.exp(tw.astype(np.float64)) * w\n",
    "        h1 = np.exp(th.astype(np.float64)) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "\n",
    "        x1 = np.round(x1)\n",
    "        y1 = np.round(y1)\n",
    "        w1 = np.round(w1)\n",
    "        h1 = np.round(h1)\n",
    "        return np.stack([x1, y1, w1, h1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return X\n",
    "\n",
    "# aplica la correccion de los deltas predichos por el modelo clasificador final\n",
    "def apply_regr_classfinal(x, y, w, h, tx, ty, tw, th):\n",
    "    # corrige las coordenadas (x,y,w,h) del anchor según los deltas (tx,ty,tw,th)\n",
    "    # tx=(cx_gt-cx_anchor)/w_anchor, ty=(cy_gt-cy_anchor)/h_anchor, tw=log(w_gt/w_anchor), tw=log(h_gt/h_anchor)\n",
    "    try:\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "\n",
    "        w1 = math.exp(tw) * w\n",
    "        h1 = math.exp(th) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        w1 = int(round(w1))\n",
    "        h1 = int(round(h1))\n",
    "\n",
    "        return x1, y1, w1, h1\n",
    "\n",
    "    except ValueError:\n",
    "        return x, y, w, h\n",
    "    except OverflowError:\n",
    "        return x, y, w, h\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x, y, w, h\n",
    "\n",
    "# define los ROIs a partir de las predicciones de scores y deltas de cada anchor por el modelo RPN\n",
    "def rpn_to_roi(out_rpn_cls, out_rpn_regr, C, max_boxes=300, overlap_thresh=0.9):\n",
    "    # Pasos:\n",
    "    #   1. Calcula los bboxes de los ROIs: obtiene coordenadas de los anchores de cada punto del feature map\n",
    "    #   2. Cada anchor es corregido por los deltas predichos por el modelo RPN\n",
    "    #   3. Recorta aquellos bboxes que sobresalgan de la imagen\n",
    "    #   4. Aplica NMS sobre los bboxes\n",
    "    # Devuelve las coordenadas de los bboxes seleccionados (no los scores)\n",
    "\n",
    "    # Decodificación deltas (deltas = deltas*0.25) - p.e. x=(x_gt-x_anc)/(w_anc*var) y w=ln(w_gt/w_anc)/var\n",
    "    out_rpn_regr = out_rpn_regr / C.std_scaling\n",
    "\n",
    "    anchor_sizes = C.anchor_box_scales   # (son 3)\n",
    "    anchor_ratios = C.anchor_box_ratios  # (son 3)\n",
    "\n",
    "    assert out_rpn_cls.shape[0] == 1\n",
    "    (rows, cols) = out_rpn_cls.shape[1:3]\n",
    "\n",
    "    # A.shape = (4, feature_map.height, feature_map.width, num_anchors) = (4,18,25,9) si la imagen es 400x300\n",
    "    # A almacena las coordenadas de los 9 anchores por cada punto del feature map => 18x25x9=4050 anchores\n",
    "    A = np.zeros((4, out_rpn_cls.shape[1], out_rpn_cls.shape[2], out_rpn_cls.shape[3]))\n",
    "\n",
    "    curr_anchor = 0 # indica un anchor en el rango 0~8 (9 anchores)\n",
    "    for anchor_size in anchor_sizes:\n",
    "        for anchor_ratio in anchor_ratios:\n",
    "            # ancho y alto del anchor en el feature map = (ancho * escala) / 16\n",
    "            anchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
    "            anchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
    "\n",
    "            # regr almacena los deltas del current_anchor en todas las posiciones del feature map\n",
    "            regr = out_rpn_regr[0, :, :, 4 * curr_anchor:4 * curr_anchor + 4] # shape => (18, 25, 4)\n",
    "            regr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
    "\n",
    "            # Grid del mismo tamaño que el feature map\n",
    "            X, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "            # Calcula coordenadas (x,y,w,h) del current_anchor en todas las posiciones del feature map\n",
    "            A[0, :, :, curr_anchor] = X - anchor_x/2\n",
    "            A[1, :, :, curr_anchor] = Y - anchor_y/2\n",
    "            A[2, :, :, curr_anchor] = anchor_x\n",
    "            A[3, :, :, curr_anchor] = anchor_y\n",
    "\n",
    "            # corrige coordenadas (x,y,w,h) del anchor con deltas (tx,ty,tw,th) predecidos por el modelo RPN\n",
    "            A[:, :, :, curr_anchor] = apply_regr_rpn(A[:, :, :, curr_anchor], regr)\n",
    "\n",
    "            # Evita bboxes con altura o anchura menor que 1 (redondea a 1)\n",
    "            A[2, :, :, curr_anchor] = np.maximum(1, A[2, :, :, curr_anchor])\n",
    "            A[3, :, :, curr_anchor] = np.maximum(1, A[3, :, :, curr_anchor])\n",
    "\n",
    "            # Convierte (x, y , w, h) => (x1, y1, x2, y2)\n",
    "            A[2, :, :, curr_anchor] += A[0, :, :, curr_anchor]\n",
    "            A[3, :, :, curr_anchor] += A[1, :, :, curr_anchor]\n",
    "\n",
    "            # Recorta aquellos bboxes que sobresalgan de la imagen (o del feature map)\n",
    "            A[0, :, :, curr_anchor] = np.maximum(0, A[0, :, :, curr_anchor])\n",
    "            A[1, :, :, curr_anchor] = np.maximum(0, A[1, :, :, curr_anchor])\n",
    "            A[2, :, :, curr_anchor] = np.minimum(cols-1, A[2, :, :, curr_anchor])\n",
    "            A[3, :, :, curr_anchor] = np.minimum(rows-1, A[3, :, :, curr_anchor])\n",
    "\n",
    "            curr_anchor += 1\n",
    "\n",
    "    # almacena la informacion en forma de listas\n",
    "    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape => (4050, 4)\n",
    "    all_probs = out_rpn_cls.transpose((0, 3, 1, 2)).reshape((-1))                 # shape => (4050,)\n",
    "\n",
    "    x1 = all_boxes[:, 0]\n",
    "    y1 = all_boxes[:, 1]\n",
    "    x2 = all_boxes[:, 2]\n",
    "    y2 = all_boxes[:, 3]\n",
    "\n",
    "    # Elimina bboxes con coordenadas erróneas\n",
    "    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "    all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "    all_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "    # Non_max_suppression. Solo capturamos los bboxes, no necesitamos los scores\n",
    "    result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "    return result\n",
    "\n",
    "# Redimensiona la imagen al tamaño especificado en la configuracion\n",
    "def format_img_size(img, C):\n",
    "#    img_min_side = float(C.im_size)\n",
    "#    (height,width,_) = img.shape\n",
    "\n",
    "#    if width <= height:\n",
    "#        ratio = img_min_side/width\n",
    "#        new_height = int(ratio * height)\n",
    "#        new_width = int(img_min_side)\n",
    "#    else:\n",
    "#        ratio = img_min_side/height\n",
    "#        new_width = int(ratio * width)\n",
    "#        new_height = int(img_min_side)\n",
    "#    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    ratio = 1\n",
    "    return img, ratio\n",
    "\n",
    "# BGR >> RGB\n",
    "def format_img_channels(img):\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def format_img(img, C):\n",
    "    img, ratio = format_img_size(img, C)\n",
    "    img = format_img_channels(img)\n",
    "    return img, ratio\n",
    "\n",
    "# Transforma las coordenadas de los bboxes de la imagen redimensionada a la original\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7685b6d",
   "metadata": {},
   "source": [
    "## Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f66fde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\TFG-PabloHernandez-Detector\\Modelos\\TFG\\ModeloTGCfull\n",
      "Loading weights from ./model/model_frcnn_vgg.hdf5\n",
      "{0: 'dorsal', 1: 'bg'}\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# TEST\n",
    "##########\n",
    "\n",
    "%cd {BASE_PATH}\n",
    "\n",
    "with open(CONFIG, 'rb') as f_in:\n",
    "\tC = pickle.load(f_in)\n",
    " \n",
    "# capa Input del modelo VGG (Imagenes RGB)\n",
    "img_input = Input(shape=(None, None, 3))\n",
    "# capa Input del modelo RoI Pooling\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "# capa Input del modelo clasificador (convolutional feature map (H/stride, W/stride, 512))\n",
    "num_features = 512\n",
    "feature_map_input = Input(shape=(None, None, num_features))\n",
    "\n",
    "# define la red base (VGG16)\n",
    "shared_layers = nn_base(img_input)\n",
    "\n",
    "# define el modelo RPN\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "# define el modelo clasificador final\n",
    "classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n",
    "\n",
    "# Creamos los modelos\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "# se intercambian las parejas <'clase', valor>\n",
    "class_mapping = C.class_mapping\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "\n",
    "imgs_path = os.listdir(TEST_IMAGES)\n",
    "all_imgs = []\n",
    "classes = {}\n",
    "\n",
    "# threshold score (se ignoran las predicciones con valores de probabilidad menores)\n",
    "bbox_threshold = 0.7\n",
    "\n",
    "# las predicciones se alamcenan en un dataframe\n",
    "column_names = [\"name\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class\", \"score\"]\n",
    "predictions = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# lee annotationTest.txt en un Dataframe para marcar los ground-truth bboxes en las imagenes\n",
    "df_bboxes_gt_test = pd.read_csv(TEST_IMAGES + '/annotateTest.txt', sep=\",\", header=None)\n",
    "df_bboxes_gt_test.columns = [\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class\"]\n",
    "# agrupa las anotaciones de una misma imagen (columna \"filename\")\n",
    "gt_grouped_by_filename = df_bboxes_gt_test.groupby('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c977d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arucas_frame_01_06_29_000.jpg\n",
      "F:/Datasets/FasterRCNNResults//Arucas_frame_01_06_29_000.jpg\n",
      "[('dorsal', 98.15324544906616), ('dorsal', 97.39568829536438), ('dorsal', 97.3118543624878), ('dorsal', 97.01340794563293), ('dorsal', 96.98181748390198), ('dorsal', 96.94610834121704), ('dorsal', 95.60053944587708), ('dorsal', 95.5707311630249), ('dorsal', 95.52544355392456), ('dorsal', 94.75155472755432), ('dorsal', 94.57700848579407), ('dorsal', 94.26361918449402), ('dorsal', 93.99100542068481), ('dorsal', 93.83662939071655), ('dorsal', 93.60968470573425), ('dorsal', 93.04572343826294), ('dorsal', 92.55263209342957), ('dorsal', 92.30912327766418), ('dorsal', 92.28964447975159), ('dorsal', 92.23117232322693), ('dorsal', 92.06234812736511), ('dorsal', 91.94070100784302), ('dorsal', 91.918283700943), ('dorsal', 91.89455509185791), ('dorsal', 91.65550470352173), ('dorsal', 91.41327738761902), ('dorsal', 91.2253737449646), ('dorsal', 91.1083996295929), ('dorsal', 91.05907678604126), ('dorsal', 90.59271216392517), ('dorsal', 90.49520492553711), ('dorsal', 90.352463722229), ('dorsal', 90.28441905975342), ('dorsal', 89.99602794647217), ('dorsal', 89.89790081977844), ('dorsal', 89.79560136795044), ('dorsal', 89.7387444972992), ('dorsal', 89.73866701126099), ('dorsal', 89.73830938339233), ('dorsal', 89.33546543121338), ('dorsal', 89.03911113739014), ('dorsal', 88.8985812664032), ('dorsal', 88.687264919281), ('dorsal', 88.53572607040405), ('dorsal', 88.31676244735718), ('dorsal', 88.01220655441284), ('dorsal', 87.98126578330994), ('dorsal', 87.92284727096558), ('dorsal', 87.90072798728943), ('dorsal', 87.41615414619446), ('dorsal', 87.34767436981201), ('dorsal', 87.2817873954773), ('dorsal', 87.28110790252686), ('dorsal', 87.277090549469), ('dorsal', 87.14428544044495), ('dorsal', 87.08423376083374), ('dorsal', 86.91093921661377), ('dorsal', 86.86143159866333), ('dorsal', 86.8509829044342), ('dorsal', 86.67203783988953), ('dorsal', 86.40542030334473), ('dorsal', 86.35810017585754), ('dorsal', 86.34241223335266), ('dorsal', 86.26283407211304), ('dorsal', 86.20426654815674), ('dorsal', 86.12339496612549), ('dorsal', 86.03832721710205), ('dorsal', 86.02932095527649), ('dorsal', 86.01478338241577), ('dorsal', 85.97477674484253), ('dorsal', 85.97121834754944), ('dorsal', 85.96159815788269), ('dorsal', 85.86999177932739), ('dorsal', 85.85919141769409), ('dorsal', 85.85619926452637), ('dorsal', 85.79326272010803), ('dorsal', 85.7813835144043), ('dorsal', 85.7569694519043), ('dorsal', 85.60780882835388), ('dorsal', 85.52834987640381), ('dorsal', 85.4827880859375), ('dorsal', 85.48228144645691), ('dorsal', 85.43014526367188), ('dorsal', 85.37276983261108), ('dorsal', 85.34499406814575), ('dorsal', 85.2052628993988), ('dorsal', 85.16771793365479), ('dorsal', 85.13636589050293), ('dorsal', 85.06960868835449), ('dorsal', 85.02353429794312), ('dorsal', 84.98291969299316), ('dorsal', 84.81617569923401), ('dorsal', 84.69159603118896), ('dorsal', 84.52268242835999), ('dorsal', 84.47123765945435), ('dorsal', 84.40894484519958), ('dorsal', 84.40113067626953), ('dorsal', 84.21446681022644), ('dorsal', 84.17418003082275), ('dorsal', 83.87508988380432), ('dorsal', 83.62670540809631), ('dorsal', 83.45173597335815), ('dorsal', 83.35682153701782), ('dorsal', 83.12678337097168)]\n",
      "Arucas_frame_01_08_02_000.jpg\n",
      "F:/Datasets/FasterRCNNResults//Arucas_frame_01_08_02_000.jpg\n",
      "[('dorsal', 98.56966733932495), ('dorsal', 98.29815030097961), ('dorsal', 97.74819016456604), ('dorsal', 97.54430055618286), ('dorsal', 97.0514714717865), ('dorsal', 97.02914357185364), ('dorsal', 96.93259596824646), ('dorsal', 96.3634729385376), ('dorsal', 96.05541825294495), ('dorsal', 95.99989652633667), ('dorsal', 95.92633247375488), ('dorsal', 95.70751786231995), ('dorsal', 95.66779732704163), ('dorsal', 95.30106782913208), ('dorsal', 95.2501118183136), ('dorsal', 95.217365026474), ('dorsal', 95.0474739074707), ('dorsal', 95.00137567520142), ('dorsal', 94.96603012084961), ('dorsal', 94.93916034698486), ('dorsal', 94.55709457397461), ('dorsal', 94.48382258415222), ('dorsal', 94.45715546607971), ('dorsal', 94.42422986030579), ('dorsal', 94.25893425941467), ('dorsal', 94.0823495388031), ('dorsal', 93.86059045791626), ('dorsal', 93.81777048110962), ('dorsal', 93.53665709495544), ('dorsal', 93.51741075515747), ('dorsal', 93.48452687263489), ('dorsal', 93.44472885131836), ('dorsal', 93.17080974578857), ('dorsal', 93.13057661056519), ('dorsal', 93.05892586708069), ('dorsal', 92.88777112960815), ('dorsal', 92.36153364181519), ('dorsal', 92.2953188419342), ('dorsal', 92.086261510849), ('dorsal', 91.87435507774353), ('dorsal', 91.72874093055725), ('dorsal', 91.6913092136383), ('dorsal', 91.59665107727051), ('dorsal', 91.42430424690247), ('dorsal', 91.35128855705261), ('dorsal', 91.02509617805481), ('dorsal', 90.96242785453796), ('dorsal', 90.43402671813965), ('dorsal', 90.42049050331116), ('dorsal', 90.36737084388733), ('dorsal', 90.30318856239319), ('dorsal', 90.12228846549988), ('dorsal', 90.12203812599182), ('dorsal', 89.72476720809937), ('dorsal', 89.64632153511047), ('dorsal', 89.62319493293762), ('dorsal', 89.49843049049377), ('dorsal', 89.23304080963135), ('dorsal', 89.18248414993286), ('dorsal', 89.0134871006012), ('dorsal', 88.60517144203186), ('dorsal', 88.41195106506348), ('dorsal', 87.53591775894165), ('dorsal', 86.7430031299591), ('dorsal', 86.36718392372131), ('dorsal', 86.34699583053589), ('dorsal', 86.0360860824585), ('dorsal', 85.96497178077698), ('dorsal', 85.76897382736206), ('dorsal', 85.09594202041626), ('dorsal', 84.70721244812012), ('dorsal', 83.33558440208435)]\n",
      "Arucas_frame_01_09_51_000.jpg\n",
      "F:/Datasets/FasterRCNNResults//Arucas_frame_01_09_51_000.jpg\n",
      "[('dorsal', 99.13045167922974), ('dorsal', 98.19019436836243), ('dorsal', 98.13143610954285), ('dorsal', 97.97316193580627), ('dorsal', 97.59235978126526), ('dorsal', 97.52066731452942), ('dorsal', 97.3318874835968), ('dorsal', 96.42874002456665), ('dorsal', 96.38686776161194), ('dorsal', 96.33280634880066), ('dorsal', 96.22400999069214), ('dorsal', 95.97868323326111), ('dorsal', 95.76621651649475), ('dorsal', 95.72592377662659), ('dorsal', 95.71126699447632), ('dorsal', 95.60131430625916), ('dorsal', 95.46215534210205), ('dorsal', 95.2190101146698), ('dorsal', 95.16820311546326), ('dorsal', 95.14224529266357), ('dorsal', 94.96186375617981), ('dorsal', 94.65413093566895), ('dorsal', 94.60911750793457), ('dorsal', 94.54712867736816), ('dorsal', 94.52541470527649), ('dorsal', 94.36271786689758), ('dorsal', 94.24360394477844), ('dorsal', 94.07042860984802), ('dorsal', 94.00822520256042), ('dorsal', 93.98490786552429), ('dorsal', 93.95071864128113), ('dorsal', 93.90315413475037), ('dorsal', 93.88530850410461), ('dorsal', 93.77279877662659), ('dorsal', 93.65720748901367), ('dorsal', 93.61748695373535), ('dorsal', 93.47091317176819), ('dorsal', 93.41235756874084), ('dorsal', 93.16139817237854), ('dorsal', 92.87898540496826), ('dorsal', 92.42488145828247), ('dorsal', 92.28642582893372), ('dorsal', 92.14088916778564), ('dorsal', 92.04033017158508), ('dorsal', 91.93928241729736), ('dorsal', 91.83719754219055), ('dorsal', 91.80847406387329), ('dorsal', 91.66445732116699), ('dorsal', 91.48061871528625), ('dorsal', 91.42884016036987), ('dorsal', 91.16089344024658), ('dorsal', 91.02738499641418), ('dorsal', 90.9974217414856), ('dorsal', 90.95403552055359), ('dorsal', 90.85942506790161), ('dorsal', 90.70934653282166), ('dorsal', 90.68378210067749), ('dorsal', 90.63191413879395), ('dorsal', 90.28965830802917), ('dorsal', 90.25797843933105), ('dorsal', 90.14552235603333), ('dorsal', 90.1159942150116), ('dorsal', 89.83221650123596), ('dorsal', 89.78410363197327), ('dorsal', 89.6497130393982), ('dorsal', 89.22291398048401), ('dorsal', 89.08563256263733), ('dorsal', 89.0723705291748), ('dorsal', 89.03713822364807), ('dorsal', 88.86098265647888), ('dorsal', 88.56663107872009), ('dorsal', 88.34114074707031), ('dorsal', 88.31337690353394), ('dorsal', 87.70949244499207), ('dorsal', 87.61596083641052), ('dorsal', 87.53018975257874), ('dorsal', 87.31996417045593), ('dorsal', 87.22100853919983), ('dorsal', 86.9161605834961), ('dorsal', 86.78472638130188), ('dorsal', 86.61501407623291), ('dorsal', 86.5083634853363), ('dorsal', 86.0471785068512), ('dorsal', 85.99793314933777), ('dorsal', 85.9843373298645), ('dorsal', 85.6019377708435), ('dorsal', 85.51359176635742), ('dorsal', 85.38956642150879), ('dorsal', 85.25368571281433), ('dorsal', 85.2143943309784), ('dorsal', 84.76084470748901), ('dorsal', 83.7641716003418), ('dorsal', 83.59910845756531), ('dorsal', 83.35973620414734), ('dorsal', 83.34614038467407), ('dorsal', 83.06752443313599)]\n",
      "Arucas_frame_01_12_08_000.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a9a524258be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Los valores deltas son codificados con la varianza, p.e. x=(x_gt-x_anc)/(w_anc*var) y w=ln(w_gt/w_anc)/var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# F: feature map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_rpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Corrige los anchores con las predicciones delta del modelo RPN y selecciona bboxes mediante NMS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_images = 1\n",
    "for idx, img_name in enumerate(imgs_path):\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    print(img_name)\n",
    "    st = time.time()\n",
    "\n",
    "    filepath = os.path.join(TEST_IMAGES + '/' + img_name)\n",
    "    img = cv2.imread(filepath)\n",
    "\n",
    "    # re-escala la imagen y transforma BGR -> RGB\n",
    "    X, ratio = format_img(img, C)\n",
    "\n",
    "    # Y1: probabilidad de cada anchor (de incluir un objeto) correspondiente a cada punto del feature map\n",
    "    # Y2: deltas del bbox de cada anchor correspondiente a cada punto del feature map\n",
    "    # Los valores deltas son codificados con la varianza, p.e. x=(x_gt-x_anc)/(w_anc*var) y w=ln(w_gt/w_anc)/var\n",
    "    # F: feature map\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "    # Corrige los anchores con las predicciones delta del modelo RPN y selecciona bboxes mediante NMS\n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, C, overlap_thresh=0.7)\n",
    "\n",
    "    # (x1,y1,x2,y2) => (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # almacena info de los ROI seleccionados\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        # selecciona los siguientes 4 bboxes\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            # pad R para incluir 4 ROIs que es la entrada esperada por el clasificador final\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        # F: feature maps\n",
    "        # P_cls (4x2): score de cada ROI (4 ROI de entrada) y para cada clase (incluyendo la 'bg')\n",
    "        # P_regr (4x4): deltas bbox (4 values) para cada clase y para cada ROI (4 ROI de entrada)\n",
    "        [P_cls, P_regr] = model_classifier.predict([F, ROIs])\n",
    "\n",
    "        # Calcula coordenadas bboxes en la imagen original\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignora ROI con (score<bbox_threshold) or (ROI con clase 'bg')\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or cls_num == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[cls_num]  # nombre asignado a la clase\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "            try:\n",
    "                # extrae deltas predecidos por el clasificador final para este ROI\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                # corregimos bbox del ROI\n",
    "                x, y, w, h = apply_regr_classfinal(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # almacenamos coordenadas de bboxes y scores del ROI\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    # obtenemos coordenadas de los ground-truth de la imagen de test\n",
    "    group_df = gt_grouped_by_filename.get_group(filepath[3:])\n",
    "    group_df = group_df.drop(['filename', 'class'], axis=1)\n",
    "    # dibuja gt bbox en la imagen en color verde\n",
    "    for index, row in group_df.iterrows():\n",
    "        (gt_x1, gt_y1, gt_x2, gt_y2) = row\n",
    "        cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), (0, 255, 0), 2)\n",
    "\n",
    "    # aplica NMS sobre los bboxes detectados y dibuja el resultado en la imagen\n",
    "    all_dets = [] # almacena las detecciones\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "        \n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "            # Calcula coordenadas en la imagen original y dibuja bbox detectado\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (0,0,255), 2)\n",
    "\n",
    "            textLabel = '{}: {}'.format(\"confianza\",int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            # muestra el string \"textLabel\" junto al bbox detectado\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel, cv2.FONT_HERSHEY_DUPLEX, 0.5, 1)\n",
    "            textOrg = (real_x1, real_y1)\n",
    "            xxx1 = textOrg[0] - 0\n",
    "            yyy1 = textOrg[1] + baseLine - 0\n",
    "            xxx2 = textOrg[0] + retval[0] + 0\n",
    "            yyy2 = textOrg[1] - retval[1] - 0\n",
    "            if xxx1<0 or yyy1<0 or xxx2<0 or yyy2<0:\n",
    "                textOrg = (real_x2, real_y2)\n",
    "                xxx1 = textOrg[0] - retval[0] - 0\n",
    "                yyy1 = textOrg[1] + retval[1] + 0\n",
    "                xxx2 = textOrg[0] + 0\n",
    "                yyy2 = textOrg[1] - baseLine + 0\n",
    "                textOrg = (xxx1, yyy1-baseLine)\n",
    "\n",
    "            cv2.rectangle(img, (xxx1, yyy1), (xxx2, yyy2), (0, 0, 0), 1)\n",
    "            cv2.rectangle(img, (xxx1, yyy1), (xxx2, yyy2), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "            predictions = predictions.append({\"name\":img_name, \"xmin\":real_x1, \"ymin\":real_y1, \"xmax\":real_x2, \"ymax\":real_y2, \"class\":key, \"score\":int(100*new_probs[jk])}, ignore_index=True)\n",
    "\n",
    "        # almacena la imagen en disco\n",
    "        print(RESULTS + '/' + img_name)\n",
    "        cv2.imwrite(RESULTS + '/' + img_name, img)\n",
    "\n",
    "#    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    number_images = number_images + 1\n",
    "    if ((number_images % 20) == 0):\n",
    "        print('Image {}'.format(number_images))\n",
    "        predictions.to_csv(RESULTS + '/' + \"annotateResults.txt\", header=None, index=None, sep= \",\")\n",
    "#    if all_dets:\n",
    "#        plt.figure(figsize=(10,10))\n",
    "#        plt.grid()\n",
    "#        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "#        plt.show()\n",
    "\n",
    "# almacena las predicciones existentes en el dataframe a un archivo\n",
    "predictions.to_csv(RESULTS + '/' + \"annotateResults.txt\", header=None, index=None, sep= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911d0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
